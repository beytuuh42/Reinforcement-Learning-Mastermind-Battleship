{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The algorithm/agent which should be used is chosen in the cell below.\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from enum import Enum, auto\n",
    "\n",
    "import tensorflow as tf\n",
    "import reverb\n",
    "\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.agents.ppo import ppo_clip_agent\n",
    "from tf_agents.agents.reinforce import reinforce_agent\n",
    "from tf_agents.environments import TFPyEnvironment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.networks import mask_splitter_network\n",
    "from tf_agents.networks import value_network\n",
    "from tf_agents.metrics import py_metrics\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.policies import random_py_policy, random_tf_policy\n",
    "from tf_agents.replay_buffers import reverb_replay_buffer\n",
    "from tf_agents.replay_buffers import reverb_utils\n",
    "from tf_agents.train.utils import spec_utils\n",
    "from tf_agents.train.utils import train_utils\n",
    "from tf_agents.train import actor\n",
    "from tf_agents.train import learner\n",
    "from tf_agents.train import ppo_learner\n",
    "from tf_agents.train import triggers\n",
    "from tf_agents.utils import common\n",
    "\n",
    "# add parent dir of agents dir to path, so the dir of the envs can be accessed\n",
    "module_path = os.path.abspath(os.path.join('./..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from environments.mastermind_v2 import MastermindEnvironment\n",
    "from environments.battleship_v2 import BattleshipEnvironment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Algorithm(Enum):\n",
    "    DQN = auto()\n",
    "    DDQN = auto()\n",
    "    PPO = auto()\n",
    "    Reinforce = auto()\n",
    "\n",
    "\n",
    "collect_env  = MastermindEnvironment()\n",
    "eval_env  = MastermindEnvironment()\n",
    "observation_spec, action_spec, time_step_spec = (spec_utils.get_tensor_specs(collect_env))\n",
    "env_name = type(collect_env).__name__\n",
    "\n",
    "algorithm = Algorithm.PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replay buffer\n",
    "replay_buffer_capacity = 20_000\n",
    "num_timesteps_per_train_call = 2\n",
    "\n",
    "# training/evaluation\n",
    "num_training_iterations = 10\n",
    "initial_collect_steps = 1_000\n",
    "num_eval_episodes = 50\n",
    "batch_size = 1\n",
    "num_epochs = 1\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# summary\n",
    "eval_interval = int(num_training_iterations * 0.05)\n",
    "log_interval = int(num_training_iterations * 0.05)\n",
    "log_dir = \"./logs/\" + type(collect_env).__name__ + '/' + algorithm.name\n",
    "\n",
    "train_step = train_utils.create_train_step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an agent of respective algorithm\n",
    "\n",
    "# function for splitting the observation into 'observation' and mask of 'valid_actions'\n",
    "splitter_fn = lambda obs: (obs['observation'], obs['valid_actions'])\n",
    "\n",
    "\n",
    "use_action_mask = True\n",
    "\n",
    "\n",
    "if use_action_mask:\n",
    "    observation_spec = observation_spec[\"observation\"]\n",
    "\n",
    "if algorithm == Algorithm.DQN:\n",
    "    \n",
    "    q_net = q_network.QNetwork(\n",
    "        observation_spec,\n",
    "        action_spec,\n",
    "        fc_layer_params=(20,100)\n",
    "    )\n",
    "\n",
    "    agent = dqn_agent.DqnAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        q_net,\n",
    "        optimizer,\n",
    "        observation_and_action_constraint_splitter=splitter_fn if use_action_mask else None,\n",
    "        train_step_counter=train_step,\n",
    "        gamma=0.9\n",
    "    )\n",
    "\n",
    "elif algorithm == Algorithm.DDQN:\n",
    "    \n",
    "    q_net = q_network.QNetwork(\n",
    "        observation_spec,\n",
    "        action_spec,\n",
    "        fc_layer_params=(20,100)\n",
    "    )\n",
    "\n",
    "    agent = dqn_agent.DdqnAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        q_net,\n",
    "        optimizer,\n",
    "        observation_and_action_constraint_splitter=splitter_fn if use_action_mask else None,\n",
    "        train_step_counter=train_step,\n",
    "        gamma=0.9\n",
    "    )\n",
    "\n",
    "elif algorithm == Algorithm.PPO:\n",
    "\n",
    "    # wrapping the network with a MaskSplitterNetwork so the observation can be splitted\n",
    "    actor_net = mask_splitter_network.MaskSplitterNetwork(\n",
    "        splitter_fn,\n",
    "        actor_distribution_network.ActorDistributionNetwork(\n",
    "            observation_spec,\n",
    "            action_spec\n",
    "        ),\n",
    "        passthrough_mask=True\n",
    "    )\n",
    "\n",
    "    # add mask argument to value_network\n",
    "    value_net = mask_splitter_network.MaskSplitterNetwork(\n",
    "        splitter_fn,\n",
    "        value_network.ValueNetwork(\n",
    "            observation_spec\n",
    "        ),\n",
    "        passthrough_mask=True\n",
    "    )\n",
    "\n",
    "    agent = ppo_clip_agent.PPOClipAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        optimizer,\n",
    "        actor_net=actor_net,\n",
    "        value_net=value_net,\n",
    "        train_step_counter=train_step,\n",
    "        update_normalizers_in_train=False,\n",
    "        num_epochs=num_epochs,\n",
    "        normalize_observations=False, #drastically reduces training\n",
    "        normalize_rewards=False,\n",
    "        use_gae=True,\n",
    "        entropy_regularization=0.001,\n",
    "        importance_ratio_clipping=0.2,\n",
    "        discount_factor=0.7,\n",
    "    )\n",
    "\n",
    "elif algorithm == Algorithm.Reinforce:\n",
    "\n",
    "    actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "        observation_spec,\n",
    "        action_spec,\n",
    "        fc_layer_params=(150,)\n",
    "    )\n",
    "    if use_action_mask:\n",
    "        actor_net = mask_splitter_network.MaskSplitterNetwork(\n",
    "            splitter_fn,\n",
    "            actor_net,\n",
    "            passthrough_mask=True,\n",
    "            \n",
    "        )\n",
    "    agent = reinforce_agent.ReinforceAgent(\n",
    "        time_step_spec,\n",
    "        action_spec,\n",
    "        actor_network=actor_net,\n",
    "        optimizer=optimizer,\n",
    "        train_step_counter=train_step,\n",
    "        gamma=0.9\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Error: Unknown algorithm\")\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize replay buffer for experience collection\n",
    "\n",
    "training_table = 'training_table'\n",
    "normalization_table = 'normalization_table'\n",
    "\n",
    "tables = [None]\n",
    "\n",
    "# initiliaze the table for experiences\n",
    "tables[0] = reverb.Table(\n",
    "    name=training_table,\n",
    "    max_size=replay_buffer_capacity,\n",
    "    sampler=reverb.selectors.Uniform(),\n",
    "    remover=reverb.selectors.Fifo(),\n",
    "    rate_limiter=reverb.rate_limiters.MinSize(1)\n",
    ")\n",
    "\n",
    "if algorithm == Algorithm.PPO:\n",
    "    # PPO uses a second replay buffer to collect experience for normalization\n",
    "    # TODO: elaborate\n",
    "    table = reverb.Table(\n",
    "        name=normalization_table,\n",
    "        sampler=reverb.selectors.Uniform(),\n",
    "        remover=reverb.selectors.Fifo(),\n",
    "        rate_limiter=reverb.rate_limiters.MinSize(1),\n",
    "        max_size=replay_buffer_capacity,\n",
    "        max_times_sampled=1,\n",
    "    )\n",
    "    tables.append(table)\n",
    "\n",
    "reverb_server = reverb.Server(tables)\n",
    "\n",
    "# initiliaze replay buffer for collection training experience\n",
    "replay_buffer_train = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "    agent.collect_data_spec,\n",
    "    table_name=training_table,\n",
    "    sequence_length=num_timesteps_per_train_call if algorithm != Algorithm.Reinforce else None,\n",
    "    local_server=reverb_server\n",
    ")\n",
    "\n",
    "if algorithm == Algorithm.PPO:\n",
    "    # initialize second replay buffer to collect experience for normalization\n",
    "    replay_buffer_normalization = reverb_replay_buffer.ReverbReplayBuffer(\n",
    "        agent.collect_data_spec,\n",
    "        table_name=normalization_table,\n",
    "        sequence_length=num_timesteps_per_train_call,\n",
    "        local_server=reverb_server\n",
    "    )\n",
    "\n",
    "    # Identical to ReverbAddTrajectoryObserver, but sequences are not cut when a boundary trajectory is seen.\n",
    "    rb_observer = reverb_utils.ReverbTrajectorySequenceObserver(\n",
    "        replay_buffer_train.py_client, \n",
    "        table_name=[training_table, normalization_table],\n",
    "        sequence_length=num_timesteps_per_train_call,\n",
    "    )\n",
    "\n",
    "    normalization_dataset = replay_buffer_normalization.as_dataset(\n",
    "        sample_batch_size=batch_size,\n",
    "        num_steps=num_timesteps_per_train_call\n",
    "    ).prefetch(4) \n",
    "    \n",
    "    normalization_dataset_fn = lambda: normalization_dataset\n",
    "\n",
    "elif algorithm == Algorithm.Reinforce:\n",
    "    # The Reinforce agent uses an observer that stores entire episodes.\n",
    "    # Timesteps are cached at every environment step and written at the end of an episode.\n",
    "\n",
    "    rb_observer = reverb_utils.ReverbAddEpisodeObserver(\n",
    "        replay_buffer_train.py_client,\n",
    "        table_name=training_table,\n",
    "        max_sequence_length=replay_buffer_capacity,\n",
    "    )\n",
    "else:\n",
    "    # The DQN agent uses an observer that stores trajectories.\n",
    "    # Time steps are cached at every environment step and written when 'sequence_length' time steps are collected.\n",
    "\n",
    "    rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
    "        py_client=replay_buffer_train.py_client,\n",
    "        table_name=training_table,\n",
    "        sequence_length=num_timesteps_per_train_call,\n",
    "    )\n",
    "\n",
    "# get entries from the replay buffer \n",
    "experience_dataset = replay_buffer_train.as_dataset(\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=num_timesteps_per_train_call if algorithm != Algorithm.Reinforce else None\n",
    ").prefetch(4)\n",
    "\n",
    "experience_dataset_fn = lambda: experience_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize policies that are used for training and evaluation\n",
    "# wrapping the agent's policies in 'PyTFEagerPolicy' for faster execution\n",
    "\n",
    "random_policy = random_py_policy.RandomPyPolicy(time_step_spec, action_spec)\n",
    "\n",
    "tf_collect_policy = agent.collect_policy\n",
    "collect_policy = py_tf_eager_policy.PyTFEagerPolicy(tf_collect_policy, use_tf_function=True)\n",
    "\n",
    "tf_eval_policy = agent.policy\n",
    "eval_policy = py_tf_eager_policy.PyTFEagerPolicy(tf_eval_policy, use_tf_function=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and run an actor that is used for initial data collection with a random policy\n",
    "\n",
    "initial_collect_actor = actor.Actor(\n",
    "    collect_env,\n",
    "    random_policy,\n",
    "    train_step,\n",
    "    steps_per_run=initial_collect_steps,\n",
    "    observers=[rb_observer]\n",
    ")\n",
    "\n",
    "#initial_collect_actor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an actor that manages interactions between a policy and the environment. \n",
    "# This actor is used for experience collection during trainng with the agent's predefined collect policy.\n",
    "# 'episodes_per_run' is used for Reinforce algorithm, because the algorithm samples entire episodes. The others use 'steps_per_run'.\n",
    "\n",
    "env_step_metric = py_metrics.EnvironmentSteps()\n",
    "\n",
    "collect_actor = actor.Actor(\n",
    "    collect_env,\n",
    "    collect_policy,\n",
    "    train_step,\n",
    "    steps_per_run=num_timesteps_per_train_call if algorithm != Algorithm.Reinforce else None,\n",
    "    episodes_per_run=num_timesteps_per_train_call if algorithm == Algorithm.Reinforce else None,\n",
    "    metrics=actor.collect_metrics(10),\n",
    "    summary_dir=os.path.join(log_dir, learner.TRAIN_DIR),\n",
    "    summary_interval=log_interval,\n",
    "    observers=[rb_observer, env_step_metric]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize an actor that is used to evaluate the agent during training \n",
    "\n",
    "eval_actor = actor.Actor(\n",
    "    eval_env,\n",
    "    eval_policy,\n",
    "    train_step,\n",
    "    episodes_per_run=num_eval_episodes,\n",
    "    metrics=actor.eval_metrics(num_eval_episodes),\n",
    "    summary_dir=os.path.join(log_dir, 'eval'),\n",
    "    summary_interval=log_interval\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a learner instance the manages all the learning details during training of the agent.action_spec\n",
    "# Additionally, simplifies logging of metrics and saving of results, by creating checkpoints.\n",
    "\n",
    "# save policy and steps-per-second metric\n",
    "learning_triggers = [\n",
    "    triggers.PolicySavedModelTrigger(\n",
    "        os.path.join(log_dir, learner.POLICY_SAVED_MODEL_DIR),\n",
    "        agent,\n",
    "        train_step,\n",
    "        log_interval\n",
    "    ),\n",
    "    triggers.StepPerSecondLogTrigger(\n",
    "        train_step, \n",
    "        log_interval\n",
    "    ),\n",
    "]\n",
    "\n",
    "# ppo has it's own implementation of a learner instance, due to it's second replay buffer?\n",
    "\n",
    "if algorithm == Algorithm.PPO:\n",
    "    agent_learner = ppo_learner.PPOLearner(\n",
    "        root_dir=log_dir,\n",
    "        train_step=train_step,\n",
    "        agent=agent,\n",
    "        experience_dataset_fn=experience_dataset_fn,\n",
    "        normalization_dataset_fn=normalization_dataset_fn,\n",
    "        summary_interval=log_interval,\n",
    "        checkpoint_interval=log_interval,\n",
    "        triggers=learning_triggers,\n",
    "        num_samples=1\n",
    "    )\n",
    "else:\n",
    "    agent_learner = learner.Learner(\n",
    "        root_dir=log_dir,\n",
    "        train_step=train_step,\n",
    "        agent=agent,\n",
    "        experience_dataset_fn=experience_dataset_fn,\n",
    "        summary_interval=log_interval,\n",
    "        checkpoint_interval=log_interval,\n",
    "        triggers=learning_triggers,\n",
    "        max_checkpoints_to_keep=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "#%tensorboard --logdir {log_dir.replace(\"/\" + algorithm.name, \"\")} --bind_all\n",
    "# alternatively open tensorboard in a separate browser tab by going to http://localhost:6006/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment out, so when continuing training, tensorboard doesn't create a new graph, but continues from run (which is automatically loaded)\n",
    "# agent.train_step_counter.assign(0)\n",
    "\n",
    "# run training for 'num_training_iterations' iterations\n",
    "# during training, the actor collects experience and the agent learns via the learner instance\n",
    "# additionally, for some algorithms the storages are flushed/cleared.\n",
    "\n",
    "\n",
    "for _ in range(num_training_iterations):\n",
    "    collect_actor.run()\n",
    "\n",
    "    if algorithm == Algorithm.PPO:\n",
    "        rb_observer.flush()\n",
    "\n",
    "    total_loss = agent_learner.run()\n",
    "\n",
    "    if algorithm == Algorithm.PPO or algorithm == Algorithm.Reinforce:\n",
    "        replay_buffer_train.clear()\n",
    "        if algorithm == Algorithm.PPO:\n",
    "            replay_buffer_normalization.clear()\n",
    "\n",
    "    step = agent_learner.train_step_numpy\n",
    "\n",
    "    if eval_interval and step % eval_interval == 0:\n",
    "        eval_actor.run_and_log()\n",
    "\n",
    "    if log_interval and step % log_interval == 0:       \n",
    "        avg_return = next(m.result() for m in eval_actor.metrics if m.name == \"AverageReturn\")\n",
    "\n",
    "        print(f\"step {step}\")\n",
    "        print(f\"\\tloss = {total_loss.loss.numpy():.2f}\")\n",
    "        print(f\"\\tavg_reward = {avg_return}\")\n",
    "\n",
    "rb_observer.close()\n",
    "reverb_server.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rendering the result of the training\n",
    "\n",
    "def render(env, num_episodes=1, delay=.5):\n",
    "    policy = eval_actor.policy\n",
    "    avg_run = 0\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        time_step = env.reset()\n",
    "        j = 0\n",
    "        \n",
    "        while not time_step.is_last():\n",
    "            j+= 1\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = env.step(action_step.action)\n",
    "            env.render(delay)\n",
    "        \n",
    "        avg_run += j\n",
    "    print(f\"Avg run: {avg_run/num_episodes}\")\n",
    "\n",
    "\n",
    "render(eval_env, num_episodes=1, delay=0.35)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
